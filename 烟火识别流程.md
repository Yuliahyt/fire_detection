# win10下利用keras-yolov3进行火焰识别操作流程 #
### 在ubuntu安装相关环境后可直接使用本程序 
### 一定要注意路径！！！ 
## *环境准备：anaconda3 、 pycharm专业版 、 github上keras-yolov3源码、要识别的数据集##
 
1. anaconda3在环境兼容方面十分优越，pycharm在运行工程文件方面比较友好，舒适感强，能支持远程跑数据，所以个人选择用pycharm连接anaconda构建的虚拟环境进行火焰识别。

2. anaconda3安装教程：[https://blog.csdn.net/weixin_43715458/article/details/100096496](https://blog.csdn.net/weixin_43715458/article/details/100096496)
安装后创建一个新环境，比如起名tf,然后就可以愉快地装各种包啦

3. pycharm专业版安装教程及小插件延续寿命：[https://shimo.im/docs/pXwjwwdYyqK89q9p/read](http://https://shimo.im/docs/pXwjwwdYyqK89q9p/read)有一些激活到2089年的教程，不过我没有运行成功，大家可以去试试。上面的方法亲测有效。

4. pycharm中使用anaconda部署环境教程：[https://blog.csdn.net/qq_29883591/article/details/78077244](https://blog.csdn.net/qq_29883591/article/details/78077244)记得使用已有环境existing environment,不要傻傻创建新环境啦

5. anaconda中tensorflow和keras的版本要对应，不然后续可能出现代码不兼容。对应版本号：[https://www.cnblogs.com/carle-09/p/11661261.html](https://www.cnblogs.com/carle-09/p/11661261.html)万一出现版本不对应，可打开prompt界面重新安装，它会自动覆盖。

6. keras-yolov3源码：[https://github.com/qqwweee/keras-yolo3](https://github.com/qqwweee/keras-yolo3) 论文地址：[https://pjreddie.com/media/files/papers/YOLOv3.pdf](https://pjreddie.com/media/files/papers/YOLOv3.pdf)
7. 如何标注图片教程：[https://blog.csdn.net/qq_45504119/article/details/105038483](https://blog.csdn.net/qq_45504119/article/details/105038483)一张一张标还是太繁琐了，如果别人有数据集分享再好不过了。网盘里有数据集，两千多张，追求精度肯定不够，不过拿来熟悉流程还是够的。

## 至此，环境安装部分完成 ##

## *配置参数，开始训练网络 ##
此处按照两篇教程步骤进行，其中细节在下面补充：  
[https://blog.csdn.net/qq_45504119/article/details/105052478](https://blog.csdn.net/qq_45504119/article/details/105052478)  
[https://blog.csdn.net/qq_45504119/article/details/105033492](https://blog.csdn.net/qq_45504119/article/details/105033492)

1. 处理voc：yolo3.cfg文件中，不仅需要改classes的数值，还需要修改
上方convolutional里filter的值。计算方法：filter=3*（classes+5)     
如果没有修改会导致网络mismatch，最后无法识别。（插图1)

2. 生成.h5文件：cfg+weight=h5,我们打开terminal，输入python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5，就可以在model_data里找到我们的预训练权重文件啦。其实这个文件可以拿来识别，不过精确度非常低。

3. 修改yolo.py文件参数：记得改成convert后的h5文件！就是上文提到的yolo.h5。

4. 设置trainyolo.py文件：建议不用源文件train.py,直接复制链接教程里的代码进行训练。注意电脑是否自带gpu。参数设置建议：（插图2)

5. 漫长的等待...如果电脑配置够好，可能是短暂的等待哈哈

6. 使用训练好的last1.h5文件进行识别：先在yolo.py文件中修改model_path,改成last1.h5的路径，然后设置识别文件。识别图像的yolostart.py可以参照上文贴的链接教程，识别视频的代码详见yolo_video.py。  

特别要注意路径的问题，写绝对路径比较保险！
## 至此，网络训练部分结束 ##
##*模型性能评估 ##
了解检测过程中的mAP：[https://blog.csdn.net/plSong_CSDN/article/details/89459175](https://blog.csdn.net/plSong_CSDN/article/details/89459175)  
可参考教程：[https://blog.csdn.net/plsong_csdn/article/details/89502117](https://blog.csdn.net/plsong_csdn/article/details/89502117)

1. 准备计算mAP需要detection-results，ground-truth、images-optional.运行mAP_preprocess.py与yolo_detect.py即可。

2. 执行scripts/extra/convert_gt_xml.py

3. 运行main.py

4. 生成output文件夹，从中获得我们需要的性能数据。(插图3、4）

## 至此，性能评估部分结束 ##
### 烟火数据集的识别训练流程大致如此啦，完整程序详见github ###

